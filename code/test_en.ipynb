{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file illustrates how you might experiment with the HMM interface.\n",
    "You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n",
    "A notebook interface is nicer than the plain Python prompt, so we provide\n",
    "a notebook version of this file as `test_en.ipynb`, which you can open with\n",
    "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import ConditionalRandomField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.setLevel(level=logging.INFO)\n",
    "log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch working directory to the directory where the data live.  You may need to edit this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entrain)=8064  len(ensup)=4051  len(endev)=996\n"
     ]
    }
   ],
   "source": [
    "entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "print(f\"{len(entrain)=}  {len(ensup)=}  {len(endev)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 95936 tokens from ensup\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 12466 word types\n",
      "INFO : Tagset: f['W', 'J', 'N', 'C', 'V', 'I', 'D', ',', 'M', 'P', '.', 'E', 'R', '`', \"'\", 'T', '$', ':', '-', '#', 'S', 'F', 'U', 'L', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    }
   ],
   "source": [
    "known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation\n",
    "log.info(f\"Tagset: f{list(entrain.tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an HMM.  Let's do some pre-training to approximately maximize the\n",
    "regularized log-likelihood on supervised training data.  In other words, the\n",
    "probabilities at the M step will just be supervised count ratios.\n",
    "\n",
    "On each epoch, you will see two progress bars: first it collects counts from\n",
    "all the sentences (E step), and then after the M step, it evaluates the loss\n",
    "function, which is the (unregularized) cross-entropy on the training set.\n",
    "\n",
    "The parameters don't actually matter during the E step because there are no\n",
    "hidden tags to impute.  The first M step will jump right to the optimal\n",
    "solution.  The code will try a second epoch with the revised parameters, but\n",
    "the result will be identical, so it will detect convergence and stop.\n",
    "\n",
    "We arbitrarily choose λ=1 for our add-λ smoothing at the M step, but it would\n",
    "be better to search for the best value of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Hidden Markov Model (HMM)\n",
      "100%|██████████| 4051/4051 [00:05<00:00, 677.86it/s]\n",
      "INFO : Cross-entropy: 12.6445 nats (= perplexity 310045.271)\n",
      "100%|██████████| 4051/4051 [00:17<00:00, 229.56it/s]\n",
      "100%|██████████| 4051/4051 [00:05<00:00, 703.22it/s]\n",
      "INFO : Cross-entropy: 7.4505 nats (= perplexity 1720.756)\n",
      "100%|██████████| 4051/4051 [00:17<00:00, 227.89it/s]\n",
      "100%|██████████| 4051/4051 [00:05<00:00, 712.23it/s]\n",
      "INFO : Cross-entropy: 7.4505 nats (= perplexity 1720.768)\n",
      "INFO : Saved model to ensup_hmm.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Hidden Markov Model (HMM)\")\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "loss_sup = lambda model: model_cross_entropy(model, eval_corpus=ensup)\n",
    "hmm.train(corpus=ensup, loss=loss_sup, λ=1.0,\n",
    "          save_path=\"ensup_hmm.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's throw in the unsupervised training data as well, and continue\n",
    "training as before, in order to increase the regularized log-likelihood on\n",
    "this larger, semi-supervised training set.  It's now the *incomplete-data*\n",
    "log-likelihood.\n",
    "\n",
    "This time, we'll use a different evaluation loss function: we'll stop when the\n",
    "*tagging error rate* on a held-out dev set stops getting better.  Also, the\n",
    "implementation of this loss function (`viterbi_error_rate`) includes a helpful\n",
    "side effect: it logs the *cross-entropy* on the held-out dataset as well, just\n",
    "for your information.\n",
    "\n",
    "We hope that held-out tagging accuracy will go up for a little bit before it\n",
    "goes down again (see Merialdo 1994). (Log-likelihood on training data will\n",
    "continue to improve, and that improvement may generalize to held-out\n",
    "cross-entropy.  But getting accuracy to increase is harder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suke1\\Desktop\\NLP-HW6\\code\\hmm.py:713: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(path, map_location=device)\n",
      "INFO : Loaded model from ensup_hmm.pkl\n",
      "100%|██████████| 996/996 [00:01<00:00, 555.09it/s]\n",
      "INFO : Cross-entropy: 7.5995 nats (= perplexity 1997.183)\n",
      "100%|██████████| 996/996 [00:02<00:00, 418.29it/s]\n",
      "INFO : Tagging accuracy: all: 88.663%, known: 93.059%, seen: 44.108%, novel: 42.734%\n",
      "100%|██████████| 8064/8064 [00:35<00:00, 226.46it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 549.54it/s]\n",
      "INFO : Cross-entropy: 7.3486 nats (= perplexity 1553.988)\n",
      "100%|██████████| 996/996 [00:02<00:00, 410.17it/s]\n",
      "INFO : Tagging accuracy: all: 87.035%, known: 91.397%, seen: 45.791%, novel: 40.291%\n",
      "INFO : Saved model to entrain_hmm.pkl\n"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel.load(\"ensup_hmm.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "                                            known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, λ=1.0,\n",
    "          save_path=\"entrain_hmm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retry the above workflow where you start with a worse supervised\n",
    "model (like Merialdo).  Does EM help more in that case?  It's easiest to rerun\n",
    "exactly the code above, but first make the `ensup` file smaller by copying\n",
    "`ensup-tiny` over it.  `ensup-tiny` is only 25 sentences (that happen to cover\n",
    "all tags in `endev`).  Back up your old `ensup` and your old `*.pkl` models\n",
    "before you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed look at the first 10 sentences in the held-out corpus,\n",
    "including Viterbi tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_your_data(model, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Viterbi: {viterbi}\")\n",
    "        log.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/T added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Loss:    3/34\n",
      "INFO : Cross-entropy: 10.617969512939453 nats (= perplexity 1571.546739142578)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Loss:    4/21\n",
      "INFO : Cross-entropy: 10.876399040222168 nats (= perplexity 1879.846113700444)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/R with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Loss:    3/18\n",
      "INFO : Cross-entropy: 9.650765419006348 nats (= perplexity 803.8407277568987)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/R from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    1/13\n",
      "INFO : Cross-entropy: 9.343343734741211 nats (= perplexity 649.5713198068107)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/N _OOV_/, ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    6/25\n",
      "INFO : Cross-entropy: 10.97645378112793 nats (= perplexity 2014.8465178873744)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    2/7\n",
      "INFO : Cross-entropy: 10.526649475097656 nats (= perplexity 1475.1535604451458)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/N should/M put/V its/P money/N where/W its/P mouth/M is/V :/I _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Loss:    10/53\n",
      "INFO : Cross-entropy: 11.650132179260254 nats (= perplexity 3213.951151938371)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/F ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Loss:    6/27\n",
      "INFO : Cross-entropy: 11.45622444152832 nats (= perplexity 2809.747163946553)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    2/9\n",
      "INFO : Cross-entropy: 10.914337158203125 nats (= perplexity 1929.9354073791285)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Loss:    2/3\n",
      "INFO : Cross-entropy: 11.67041015625 nats (= perplexity 3259.4429962987115)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(hmm, endev, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try supervised training of a CRF (this doesn't use the unsupervised\n",
    "part of the data, so it is comparable to the supervised pre-training we did\n",
    "for the HMM).  We will use SGD to approximately maximize the regularized\n",
    "log-likelihood. \n",
    "\n",
    "As with the semi-supervised HMM training, we'll periodically evaluate the\n",
    "tagging accuracy (and also print the cross-entropy) on a held-out dev set.\n",
    "We use the default `eval_interval` and `tolerance`.  If you want to stop\n",
    "sooner, then you could increase the `tolerance` so the training method decides\n",
    "sooner that it has converged.\n",
    "\n",
    "We arbitrarily choose reg = 1.0 for L2 regularization, learning rate = 0.05,\n",
    "and a minibatch size of 10, but it would be better to search for the best\n",
    "value of these hyperparameters.\n",
    "\n",
    "Note that the logger reports the CRF's *conditional* cross-entropy, log p(tags\n",
    "| words) / n.  This is much lower than the HMM's *joint* cross-entropy log\n",
    "p(tags, words) / n, but that doesn't mean the CRF is worse at tagging.  The\n",
    "CRF is just predicting less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Conditional Random Field (CRF)\n",
      "\n",
      "100%|██████████| 996/996 [00:02<00:00, 451.51it/s]\n",
      "INFO : Cross-entropy: 3.0522 nats (= perplexity 21.161)\n",
      "100%|██████████| 996/996 [00:01<00:00, 535.46it/s]\n",
      "INFO : Tagging accuracy: all: 3.299%, known: 3.329%, seen: 4.545%, novel: 2.378%\n",
      "100%|██████████| 500/500 [00:04<00:00, 122.85it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 452.11it/s]\n",
      "INFO : Cross-entropy: 0.7771 nats (= perplexity 2.175)\n",
      "100%|██████████| 996/996 [00:01<00:00, 542.39it/s]\n",
      "INFO : Tagging accuracy: all: 72.742%, known: 74.658%, seen: 53.030%, novel: 52.840%\n",
      "100%|██████████| 500/500 [00:04<00:00, 121.01it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 447.74it/s]\n",
      "INFO : Cross-entropy: 0.6302 nats (= perplexity 1.878)\n",
      "100%|██████████| 996/996 [00:01<00:00, 541.30it/s]\n",
      "INFO : Tagging accuracy: all: 77.360%, known: 79.163%, seen: 58.418%, novel: 58.785%\n",
      "100%|██████████| 500/500 [00:04<00:00, 120.01it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 447.26it/s]\n",
      "INFO : Cross-entropy: 0.5320 nats (= perplexity 1.702)\n",
      "100%|██████████| 996/996 [00:01<00:00, 567.42it/s]\n",
      "INFO : Tagging accuracy: all: 81.761%, known: 83.325%, seen: 63.805%, novel: 66.248%\n",
      "100%|██████████| 500/500 [00:04<00:00, 120.72it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 446.65it/s]\n",
      "INFO : Cross-entropy: 0.4835 nats (= perplexity 1.622)\n",
      "100%|██████████| 996/996 [00:01<00:00, 537.02it/s]\n",
      "INFO : Tagging accuracy: all: 84.191%, known: 86.090%, seen: 61.785%, novel: 65.588%\n",
      "100%|██████████| 500/500 [00:04<00:00, 121.31it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 442.06it/s]\n",
      "INFO : Cross-entropy: 0.4573 nats (= perplexity 1.580)\n",
      "100%|██████████| 996/996 [00:02<00:00, 479.63it/s]\n",
      "INFO : Tagging accuracy: all: 84.672%, known: 86.525%, seen: 64.815%, novel: 65.720%\n",
      "100%|██████████| 500/500 [00:04<00:00, 112.88it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 451.02it/s]\n",
      "INFO : Cross-entropy: 0.4305 nats (= perplexity 1.538)\n",
      "100%|██████████| 996/996 [00:01<00:00, 551.80it/s]\n",
      "INFO : Tagging accuracy: all: 85.958%, known: 87.784%, seen: 64.815%, novel: 67.900%\n",
      "100%|██████████| 500/500 [00:04<00:00, 119.05it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 445.44it/s]\n",
      "INFO : Cross-entropy: 0.4128 nats (= perplexity 1.511)\n",
      "100%|██████████| 996/996 [00:02<00:00, 495.04it/s]\n",
      "INFO : Tagging accuracy: all: 86.747%, known: 89.103%, seen: 61.785%, novel: 62.550%\n",
      "100%|██████████| 500/500 [00:04<00:00, 120.90it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 434.18it/s]\n",
      "INFO : Cross-entropy: 0.3923 nats (= perplexity 1.480)\n",
      "100%|██████████| 996/996 [00:02<00:00, 455.71it/s]\n",
      "INFO : Tagging accuracy: all: 86.279%, known: 88.306%, seen: 63.131%, novel: 66.116%\n",
      "100%|██████████| 500/500 [00:04<00:00, 118.63it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 445.37it/s]\n",
      "INFO : Cross-entropy: 0.3786 nats (= perplexity 1.460)\n",
      "100%|██████████| 996/996 [00:01<00:00, 568.39it/s]\n",
      "INFO : Tagging accuracy: all: 87.073%, known: 89.149%, seen: 62.963%, novel: 66.579%\n",
      "100%|██████████| 500/500 [00:04<00:00, 119.79it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 452.05it/s]\n",
      "INFO : Cross-entropy: 0.3724 nats (= perplexity 1.451)\n",
      "100%|██████████| 996/996 [00:01<00:00, 567.59it/s]\n",
      "INFO : Tagging accuracy: all: 87.620%, known: 89.794%, seen: 62.963%, novel: 65.918%\n",
      "100%|██████████| 500/500 [00:04<00:00, 115.71it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 441.40it/s]\n",
      "INFO : Cross-entropy: 0.3570 nats (= perplexity 1.429)\n",
      "100%|██████████| 996/996 [00:01<00:00, 549.43it/s]\n",
      "INFO : Tagging accuracy: all: 87.945%, known: 90.326%, seen: 62.458%, novel: 63.606%\n",
      "100%|██████████| 500/500 [00:04<00:00, 119.00it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 447.67it/s]\n",
      "INFO : Cross-entropy: 0.3485 nats (= perplexity 1.417)\n",
      "100%|██████████| 996/996 [00:01<00:00, 562.79it/s]\n",
      "INFO : Tagging accuracy: all: 87.432%, known: 89.593%, seen: 64.141%, novel: 65.390%\n",
      "INFO : Saved model to ensup_crf.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF)\\n\")\n",
    "crf = ConditionalRandomField(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "crf.train(corpus=ensup, loss=loss_dev, reg=0.2, lr=0.1, minibatch_size=10,\n",
    "          save_path=\"ensup_crf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the CRF does on individual sentences. \n",
    "(Do you see any error patterns here that would inspire additional CRF features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/N _OOV_/N that/I anyone/N who/W has/V eaten/N in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/N ,/, ``/` and/C that/I means/N virtually/N everyone/N who/W works/V here/R ./.\n",
      "INFO : Loss:    7/34\n",
      "INFO : Cross-entropy: 0.7523502111434937 nats (= perplexity 1.6845347979692689)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/N Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Loss:    1/21\n",
      "INFO : Cross-entropy: 0.5055727362632751 nats (= perplexity 1.4196868652153103)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/N who/W are/V aligned/N with/I the/D drug/N traffickers/N ,/, not/R the/D left/N _OOV_/N ./.\n",
      "INFO : Loss:    3/18\n",
      "INFO : Cross-entropy: 0.3454820513725281 nats (= perplexity 1.2705754595866916)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/N from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    1/13\n",
      "INFO : Cross-entropy: 0.4143294095993042 nats (= perplexity 1.3326790930041164)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/N _OOV_/N government/N _OOV_/N of/I the/D ``/` _OOV_/N ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    5/25\n",
      "INFO : Cross-entropy: 0.9537739157676697 nats (= perplexity 1.9369328026875579)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    0/7\n",
      "INFO : Cross-entropy: 0.3652220666408539 nats (= perplexity 1.2880798726336722)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/N that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/N mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/N computer/N equipment/N to/T replace/V that/I damaged/N at/I El/N _OOV_/N ,/, buy/N ad/N space/N ,/, publish/N stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Loss:    7/53\n",
      "INFO : Cross-entropy: 0.5973090529441833 nats (= perplexity 1.5128920762995157)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/I to/T ``/` sponsor/N ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/N for/I added/N security/N in/I exchange/N for/I exclusive/N stories/N ./.\n",
      "INFO : Loss:    6/27\n",
      "INFO : Cross-entropy: 0.8206638097763062 nats (= perplexity 1.7662184668890375)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/N El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    1/9\n",
      "INFO : Cross-entropy: 0.8885316848754883 nats (= perplexity 1.8512909955764507)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/N B./N Evans/.\n",
      "INFO : Loss:    1/3\n",
      "INFO : Cross-entropy: 0.6775731444358826 nats (= perplexity 1.5994469255386574)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(crf, endev, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
